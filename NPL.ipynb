{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdbb44d5-88ee-47a8-a1c2-0cccdcc9680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d370d8b-5802-4437-932f-d063dc2cbe8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny = b + Wx + U * tanh(d + Hx)\\n\\nx = concat of all input sequence feature vectors(words)\\nb = biases for W\\nd = biases for H\\nW = direct representation matrix\\nH = hidden layer matrix\\nU = another hidden to output layer matrix\\n\\ny = (Wx + b) + (U * tanh(d+Hx))\\ny =  (1,|V|) +   (1, |V|) \\n     \\ngoes to two different models, addition = (1,|V|) + (1, |V|) = (1,|V|)\\n|V| -> length of vocabuluary\\n\\nthen (1,|V|) -> softmax -> probabilities for each word in vocab\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "\"\"\"\n",
    "y = b + Wx + U * tanh(d + Hx)\n",
    "\n",
    "x = concat of all input sequence feature vectors(words)\n",
    "b = biases for W\n",
    "d = biases for H\n",
    "W = direct representation matrix\n",
    "H = hidden layer matrix\n",
    "U = another hidden to output layer matrix\n",
    "\n",
    "y = (Wx + b) + (U * tanh(d+Hx))\n",
    "y =  (1,|V|) +   (1, |V|) \n",
    "     \n",
    "goes to two different models, addition = (1,|V|) + (1, |V|) = (1,|V|)\n",
    "|V| -> length of vocabuluary\n",
    "\n",
    "then (1,|V|) -> softmax -> probabilities for each word in vocab\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a2286ce-381f-4659-90d8-a959e72f2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPL:\n",
    "\n",
    "    def __init__(self, vocab, hidden_units=100, context_size=3, feature_word_len=10, has_direct_rep=True):\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.feature_word_len = feature_word_len\n",
    "        self.has_direct_rep = has_direct_rep\n",
    "        self.context_size = context_size\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.C = torch.randn(self.vocab, feature_word_len)\n",
    "        self.hidden_layer = torch.randn((self.context_size*self.feature_word_len), self.hidden_units)\n",
    "        self.b = torch.randn(self.hidden_units)\n",
    "        self.output_layer = torch.randn(self.hidden_units, self.vocab)\n",
    "        \n",
    "        self.parameters = [self.C, self.hidden_layer, self.b, self.output_layer]\n",
    "        \n",
    "        if has_direct_rep:\n",
    "            self.direct_representation = torch.randn((self.context_size*self.feature_word_len), self.vocab)\n",
    "            self.d = torch.randn(self.vocab)\n",
    "            self.parameters.extend([self.direct_representation, self.d])\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.CLE = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Set parameters gradient to true\n",
    "        for p in self.parameters:\n",
    "            p.requires_grad = True\n",
    "            \n",
    "    # List of word indexes to feature vectors\n",
    "    def get_feature_vectors(self, x, y):\n",
    "\n",
    "        # C[[index_1,index_2,index_3],...], C[[index_y],...]\n",
    "        x,y = self.C[x], self.C[y]\n",
    "       \n",
    "        # concat all input feature vectors into one\n",
    "        x = x.view(x.shape[0], x.shape[1]*x.shape[2]) # [B, context_size*feature_vector_len)\n",
    "        \n",
    "        return x,y\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "\n",
    "        x,y = self.get_feature_vectors(x,y)\n",
    "        #print(x.shape,y.shape) # [B, context_size*feature_vector_len] , [B, feature_vector_len]\n",
    "        \n",
    "        # Hidden layer tanh(b+Hx)\n",
    "        H = self.tanh(torch.matmul(x, self.hidden_layer) + self.b)\n",
    "        O = torch.matmul(H, self.output_layer)\n",
    "\n",
    "        if self.has_direct_rep:\n",
    "            # Direct representation layer (Wx + d)\n",
    "            D = torch.matmul(x, self.direct_representation) + self.d\n",
    "            logits = O + D\n",
    "        else:\n",
    "            logits = O\n",
    "\n",
    "        return logits\n",
    "        \n",
    "    def __call__(self, x,y):\n",
    "        logits = self.forward(x,y)\n",
    "        return logits\n",
    "        \n",
    "    def generate(self, start_context, length):\n",
    "\n",
    "        if type(start_context) is not str:\n",
    "            raise \"Context has to be a string\"\n",
    "\n",
    "        start_context = start_context.split()\n",
    "\n",
    "        if len(start_context) > self.context_size:\n",
    "            prnit(\"input string larger than context size, might lead to improper responses\\n\")\n",
    "        \n",
    "        input_vectors = self.get_feature_vectors(start_context)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "392df478-5ba1-49d0-97e6-d666fa20f9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'th'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def prepare_text(filename):\n",
    "    words = open(filename, \"r\").read()\n",
    "    words = words.lower()\n",
    "    words = re.sub(r'[^a-zA-Z\\s]', '', words)\n",
    "    words = words.split()\n",
    "    \n",
    "    # Create vocabulary by removing all duplicates\n",
    "    vocab = sorted(list(set(words)))\n",
    "    \n",
    "    return words, vocab\n",
    "    \n",
    "def create_pairs(words, context_size):\n",
    "    for i in range(len(words)-context_size):\n",
    "        x.append(words[i:i+context_size])\n",
    "        y.append(words[i+context_size:i+context_size+1])\n",
    "\n",
    "    # x[i] -> [\"asd\",\"Asd\",\"aw\"] context_size=3\n",
    "    # y[i] -> [\"fgds\"]\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def get_index_vectors(x, y, words_to_i):\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[i])):\n",
    "            x[i][j] = words_to_i[x[i][j]]\n",
    "        y[i] = words_to_i[y[i][0]]\n",
    "\n",
    "    # x -> [12312,1231,1] context_size=3\n",
    "    # y -> [5]\n",
    "    return x,y\n",
    "\n",
    "def get_word_dict(vocab):\n",
    "    \n",
    "    words_to_i = {}\n",
    "    i_to_words = {}\n",
    "    \n",
    "    for i in enumerate(vocab):\n",
    "        #('word', index) <=> (index, 'word')\n",
    "        words_to_i[i[1]] = i[0]\n",
    "        i_to_words[i[0]] = i[1]\n",
    "        \n",
    "    return words_to_i, i_to_words\n",
    "    \n",
    "def train(text_file, **kwargs):\n",
    "    \n",
    "    defaults = {\n",
    "        'hidden_units': 100,\n",
    "        'context_size': 3,\n",
    "        'feature_vector_size': 10,\n",
    "        'direct_rep': False,\n",
    "    }\n",
    "\n",
    "    defaults.update(kwargs)\n",
    "\n",
    "    # Prepare data\n",
    "    words, vocab = prepare_text(text_file)\n",
    "\n",
    "    # Helper dictionaries mapping words to index and vice versa\n",
    "    words_to_i, i_to_words = get_word_dict(vocab)\n",
    "    \n",
    "    x,y = create_pairs(words, defaults['context_size'])\n",
    "    x,y = get_index_vectors(x,y, words_to_i)\n",
    "    \n",
    "    # Model\n",
    "    model = NPL(vocab=len(vocab), hidden_units=defaults['hidden_units'], context_size=defaults['context_size'], \n",
    "                feature_word_len=defaults['feature_vector_size'], has_direct_rep=defaults['direct_rep'])\n",
    "\n",
    "    # optimizer and loss\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    CLE = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters, lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3413086d-648c-44a4-bec7-838dc8d820c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(filename):\n",
    "    words = open(filename, \"r\").read()\n",
    "    words = words.lower()\n",
    "    words = re.sub(r'[^a-zA-Z\\s]', '', words)\n",
    "    words = words.split()\n",
    "    vocab = list(set(words))\n",
    "    return words, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "110f4426-eeb4-4ea2-84be-ff415198ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "words,vocab = prepare_text(\"t8.shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a3fec333-41c9-4430-a3c9-e5864af0809d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(899836, 28122)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "570235b1-0a1f-451b-a896-fc072e458b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_i = {}\n",
    "i_to_words = {}\n",
    "vocab = sorted(vocab)\n",
    "for i in enumerate(vocab):\n",
    "    words_to_i[i[1]] = i[0]\n",
    "    i_to_words[i[0]] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2adcce3e-b254-4b37-baa5-09802c89747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = [], []\n",
    "context_size = 3\n",
    "feature_vector_size = 10 # 3*10 -> 30 input size\n",
    "C = torch.randn(len(vocab), feature_vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "53cbc2b0-78cc-4d3a-95f3-1c0cc67e6cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2]\n",
    "a.extend([3,4])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b66b684-9add-4e70-8be2-dce79362b991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "cc23c714-9baf-4c09-89ae-ba30f733188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(words, context_size):\n",
    "    for i in range(len(words)-context_size):\n",
    "        x.append(words[i:i+context_size])\n",
    "        y.append(words[i+context_size:i+context_size+1])\n",
    "\n",
    "    # x[i] -> [\"asd\",\"Asd\",\"aw\"]\n",
    "    # y[i] -> [\"fgds\"]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "edecfc4b-f01e-4df5-a35e-2426379fd1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_vectors(x, y, words_to_i):\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[i])):\n",
    "            x[i][j] = words_to_i[x[i][j]]\n",
    "        y[i] = words_to_i[y[i][0]]\n",
    "\n",
    "    # x -> [12312,1231,1]\n",
    "    # y -> [5]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ff46353b-7909-4c9f-b5ff-a44e856534ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = create_pairs(words, 5)\n",
    "x,y = get_index_vectors(x,y, words_to_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "33e94398-4e6f-45ac-939b-21f31e7a0203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([899831, 5]), torch.Size([899831]))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = torch.tensor(x), torch.tensor(y)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "3bb1701a-6d5c-4afe-93c9-3cc919e85f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab, hidden_units=100, context_size=3, feature_word_len=10, has_direct_rep=True):\n",
    "model = NPL(vocab=len(vocab), hidden_units=100, context_size=5, feature_word_len=10, has_direct_rep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "cadddde1-073a-45a4-8b69-8292b6fa360a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([899831, 5]), torch.Size([899831]))"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c6cbf6fd-f583-4896-bc3a-2c576cba0fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[24508, 13013, 24393, 24358,  8367]]), tensor([9204]))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:1], y[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c458f814-8b4f-4cef-a0d1-7815e702b8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "dd4d6365-9b70-4f75-8872-e60b3439dd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 28122])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "8c9660a1-3c0c-4125-9240-a12012b7e8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9204, 18779,  3341, 19033, 11001,   844, 13013, 18779, 12455,  5239])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "ea6fb29e-e7e1-4ef6-aa82-196c5ac74a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23.9045, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(x[:10],y[:10])\n",
    "loss = CLE(logits, y[:10])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "249ddc02-0829-4004-87ad-0c7bba5e1a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.9045, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec7abfa-576e-4970-9a9e-a7b9cc97e29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "828be789-4fd1-4a02-8cc1-c69f8ceeaf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters, lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "2f94cd02-b1f9-494e-baa3-fa956a629992",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "CLE = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "24180679-6e31-465c-8969-029e8badf76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3f24ffba-a18e-46af-8656-3924ae6d2e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 28122])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e904fc-2cda-4d7c-96ea-8eb35c929821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab, hidden_units=100, context_size=3, feature_word_len=10, has_direct_rep=True):\n",
    "model = NPL(vocab=len(vocab), hidden_units=100, context_size=3, feature_word_len=10, has_direct_rep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5ef5cf-6385-411d-b0b1-3cabfa41d5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e9ec1-b82c-4ab7-8d1d-5c14d3e2a8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a976f29-30b6-49a3-8868-4535a4cc3ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
