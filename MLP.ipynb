{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e19479e-a095-4b30-ad28-9ed5499a3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbfe5370-eb45-4541-a964-25bd7ec292f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'names.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines()\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'names.txt'"
     ]
    }
   ],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0cfbc63-f1f0-4ce3-9ec1-0a64c389a037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev: ...\n",
      "next: e\n",
      "prev: ..e\n",
      "next: m\n",
      "prev: .em\n",
      "next: m\n",
      "prev: emm\n",
      "next: a\n",
      "prev: mma\n",
      "next: .\n",
      "prev: ...\n",
      "next: o\n",
      "prev: ..o\n",
      "next: l\n",
      "prev: .ol\n",
      "next: i\n",
      "prev: oli\n",
      "next: v\n",
      "prev: liv\n",
      "next: i\n",
      "prev: ivi\n",
      "next: a\n",
      "prev: via\n",
      "next: .\n",
      "prev: ...\n",
      "next: a\n",
      "prev: ..a\n",
      "next: v\n",
      "prev: .av\n",
      "next: a\n",
      "prev: ava\n",
      "next: .\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "for j in range(len(words[:3])):\n",
    "    if len(words) >= 3:\n",
    "        temp = '...' + words[j] + '.'\n",
    "        for i in range(len(temp)-k):\n",
    "            print(\"prev:\", temp[i:i+k])\n",
    "            print(\"next:\", temp[i+k:i+k+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f159ad-8b60-4e8e-8e33-a32ffddb8354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = sorted(list(set(''.join(words))))\n",
    "char = ['.'] + char\n",
    "stoi = {}\n",
    "for i in range(len(char)):\n",
    "    stoi[char[i]] = i\n",
    "itoi = {}\n",
    "for i in range(len(char)):\n",
    "    itoi[i] = char[i]\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8d082abc-9c22-4113-847f-9cf604181213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '.',\n",
       " 1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z'}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41dab39-d863-4763-88ab-5e3869d3cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "x,y = [], []\n",
    "k = 3\n",
    "for j in range(len(words)):\n",
    "    temp = '...' + words[j] + '.'\n",
    "    for i in range(len(temp)-k):\n",
    "        #print(\"prev:\", temp[i:i+k])\n",
    "        #print(\"next:\", temp[i+k:i+k+1])\n",
    "        x.append([stoi[char] for char in temp[i:i+k]])\n",
    "        y.append(stoi[temp[i+k:i+k+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f733004-42ea-4fd4-b179-140ea849776b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dde10af2-91fb-4295-9a92-41a8b46abba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0]]), tensor([5]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:1],y[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f1d93e6-fc90-4f45-b523-b55ca2d9c759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C matrix\n",
    "# Feature vector length 2\n",
    "C = torch.randn((len(char),2), requires_grad=True)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "425a9588-16f9-4519-b637-9ecc9f1df3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2737,  0.4166], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "457c06c0-ea76-45b0-a92f-42c6d3a3bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one_hot = F.one_hot(x, num_classes=27).float()\n",
    "y_one_hot = F.one_hot(y, num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d2d0c50-c203-4531-b19c-38f75d3179af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_one_hot[0],y_one_hot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63e10cda-f67f-4d56-a2ac-3c1f0f40e34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2737,  0.4166],\n",
       "        [-1.2737,  0.4166],\n",
       "        [-1.2737,  0.4166]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_one_hot[0] @ C\n",
    "# Gets approriate embedding vectors from feature vector matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "634f0e49-0850-4a56-9bc2-7f065ba2560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0812d499-b9fd-4ef8-9179-a857a701890c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.9347, -0.4012],\n",
       "          [-0.9347, -0.4012],\n",
       "          [-0.9347, -0.4012]],\n",
       " \n",
       "         [[-0.9347, -0.4012],\n",
       "          [-0.9347, -0.4012],\n",
       "          [ 1.1764,  0.9330]],\n",
       " \n",
       "         [[-0.9347, -0.4012],\n",
       "          [ 1.1764,  0.9330],\n",
       "          [ 0.1516, -0.5773]]], grad_fn=<SliceBackward0>),\n",
       " torch.Size([228146, 3, 2]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[:3], emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74a7fda1-e9df-4442-8497-99557af28827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row has 3 feature vectors(words) of length 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eebf59b-6771-4e87-9ec1-5fadfa9cda52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat feature vectors into one\n",
    "emb = emb.view(emb.shape[0], emb.shape[1]*emb.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3662b35-39d5-48ef-8381-ea51e83f4b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.9347, -0.4012, -0.9347, -0.4012, -0.9347, -0.4012],\n",
       "         [-0.9347, -0.4012, -0.9347, -0.4012,  1.1764,  0.9330],\n",
       "         [-0.9347, -0.4012,  1.1764,  0.9330,  0.1516, -0.5773]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " torch.Size([228146, 6]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each ith -> 3 chars\n",
    "emb[:3], emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "94930153-d7c5-4465-a424-913d6be25cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix: torch.Size([27, 2])\n",
      "Input: torch.Size([32, 6])\n",
      "Hidden layer: torch.Size([6, 100]) Bias: torch.Size([100])\n",
      "Output layer: torch.Size([100, 27]) Bias: torch.Size([27])\n",
      "parameters: 3481\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "C = torch.randn((len(char),2), requires_grad=True)\n",
    "\n",
    "print(\"Embedding matrix:\", C.shape)\n",
    "print(\"Input:\", emb.shape)\n",
    "hidden_units = 100\n",
    "H = torch.randn(emb.shape[1], hidden_units)\n",
    "b = torch.randn(hidden_units)\n",
    "print(\"Hidden layer:\", H.shape, \"Bias:\", b.shape)\n",
    "\n",
    "output_layer = torch.randn(hidden_units, C.shape[0])\n",
    "d = torch.randn(C.shape[0]) \n",
    "print(\"Output layer:\", output_layer.shape, \"Bias:\", d.shape)\n",
    "\n",
    "parameters = [H,b,output_layer, d, C]\n",
    "print(\"parameters:\", sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "328abaf1-5087-4af6-b7fb-233a0eda05a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters: p.requires_grad=True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4305d642-aacc-446a-ad0f-5a3d82dca5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss, Softmax\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1ef24818-672b-45b8-b7da-869120ef2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Softmax(dim=1)\n",
    "CEL = CrossEntropyLoss()\n",
    "epochs = 5\n",
    "optimizer = optim.SGD([H,output_layer, b,d, C], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "50df3356-c8c2-4bf6-bc1b-8c6d8cdd0845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(43.2654, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(41.1507, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(39.2258, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(37.4627, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(35.8549, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    emb = C[x]\n",
    "    emb = emb.view(emb.shape[0], emb.shape[1]*emb.shape[2])\n",
    "    layer_1_output = emb @ H + b # First layer\n",
    "    logits = layer_1_output @ output_layer + d # Output layer\n",
    "    loss = CEL(logits, y) # Loss calculation\n",
    "    print(\"loss:\", loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() # Calculate how weights affect the loss\n",
    "    optimizer.step() # Change optimization parameters to lower loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68de0a92-605d-4880-b17b-819dcbbbc2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Generate random learning rates\n",
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10**lre\n",
    "lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "06a84145-e0ed-4f04-879e-ed42e93e29ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(45.8462, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(34.7723, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(37.2143, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(32.6861, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(41.8107, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(46.4517, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(40.1512, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(42.6002, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(34.6159, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(50.1302, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(37.0948, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(42.0197, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(35.1789, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(43.5265, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.9314, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(37.3333, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(36.2475, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(32.7522, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(30.0287, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(36.9942, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(40.1151, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(38.3982, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(36.9150, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(32.3830, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(27.8076, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(31.4839, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(36.3117, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.9293, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(33.6985, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(38.6370, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(33.5183, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(42.0828, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(37.9759, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(32.5389, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(35.4539, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(27.0260, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(36.9243, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(35.1244, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.5612, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(30.3116, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(39.9501, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.1345, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(33.1989, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(30.6654, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(37.0044, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(34.7299, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.9311, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(33.6937, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(31.0571, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(31.5163, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(33.1222, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(32.5333, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(24.4880, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(30.3827, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(26.1089, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(32.7511, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(25.8425, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(22.0110, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(22.2662, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(31.8076, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(31.2366, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(23.1655, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(25.4223, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(29.0190, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(27.0488, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(25.4878, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(27.1001, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(37.7564, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(27.0352, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(32.4050, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(24.6554, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(30.7481, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(27.6870, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(25.5901, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.0302, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(22.5783, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(26.3965, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(24.4106, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(33.0599, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(25.5768, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(22.9385, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(27.1096, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.0415, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.6643, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(21.5605, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(26.3117, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(26.4675, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(23.7106, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(24.6434, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(21.6583, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(29.2406, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(24.2618, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(26.3281, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(25.7639, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.2551, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(21.0525, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(26.2685, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(30.5648, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(30.5188, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(24.6156, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(25.0431, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.4801, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(22.2718, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(25.3187, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(20.2625, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(24.9445, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(22.5065, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(23.9073, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(26.5709, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(24.9530, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(20.5719, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(20.2378, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(24.0554, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(24.4743, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(20.1620, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.3691, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(27.3836, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.1571, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(21.0121, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(28.0655, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(25.3088, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.6420, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(20.6052, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.5489, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17.2901, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.1673, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(20.0721, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.5782, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.7055, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17.3208, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(24.3294, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.7276, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(22.7286, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.1346, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.0853, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.7609, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.3337, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.9875, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.8976, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.3928, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.1391, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(21.1404, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.4793, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(20.2189, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17.2434, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17.1324, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.9390, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.4772, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.1525, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.5073, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.5079, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17.9318, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.0446, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17.7783, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.0275, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.4328, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17.8600, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.0714, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.9316, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(18.4831, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(19.3706, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.8757, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.5995, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.2739, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.5962, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(21.7462, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.1957, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.1341, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17.1061, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.9160, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.8840, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.9070, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.9881, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(16.5568, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(17.1028, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(15.0790, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.4410, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.1369, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.9167, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.9903, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.2841, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.9156, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.6336, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.4461, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.5153, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.5300, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.0848, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.8204, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.4762, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.9794, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.4259, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.1615, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.5305, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.0263, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.7777, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.6138, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.9040, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.8958, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.3121, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.2878, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.7798, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.5010, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.8344, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.2074, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.7222, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.2516, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(14.1187, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.3527, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.0478, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.8636, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.6171, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.8202, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.6637, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(13.3448, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.7764, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.9439, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.4750, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.8503, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.3941, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.7931, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.4820, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.5559, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.1240, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.8906, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.4140, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.2428, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(12.0945, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.9123, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.5402, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.1767, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.1428, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.9476, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.7149, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.9966, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(10.1532, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.0428, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.7860, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.5209, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.6198, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.1542, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.4306, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.4450, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.7066, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.5428, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.5295, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.7824, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.0014, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.5936, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.0893, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(11.0709, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.8035, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.0177, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.0769, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.4453, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.7632, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.4649, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.4891, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.4474, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.1957, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.8388, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.2110, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.8563, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.0002, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.0495, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.5728, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.7573, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.6355, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.6407, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.0378, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.1957, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.0633, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7642, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.4228, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.2997, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.4225, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.7266, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.9870, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.3726, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.0812, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.0828, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.8972, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.6141, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.5098, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.6991, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.5513, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.0857, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.3492, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.1585, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.3994, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(9.9731, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5265, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.6423, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.2425, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.0038, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.6258, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.8575, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.5653, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.2344, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7176, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(8.3534, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4516, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7971, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.5770, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.9114, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.0905, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.1176, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.2227, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.3932, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(7.1814, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.6747, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.1589, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.0247, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.9105, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4606, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9884, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.4014, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9380, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.3970, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.4809, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.2370, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.4850, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.6782, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4446, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5839, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.9144, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.7548, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.8540, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.9537, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.4708, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.2499, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5337, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.8522, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1953, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.6493, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.5843, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.8136, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4156, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.1211, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5675, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.6310, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7468, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9078, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.0921, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1345, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4661, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.0591, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0033, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9162, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.2420, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0595, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.2869, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.4944, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.8810, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9482, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.9262, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.6191, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.1675, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7110, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5657, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4713, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4563, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.6032, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5131, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5118, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.1311, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4477, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8693, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4893, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.1755, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2343, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.4868, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3196, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1671, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.0912, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(6.8538, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4139, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2526, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1034, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7050, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.6515, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.0880, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2576, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9174, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7123, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3248, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.1441, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9475, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.6979, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5369, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2869, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5103, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2395, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0171, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1670, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4705, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4721, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8098, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9069, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1604, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9725, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5814, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1785, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5968, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5294, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5202, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3295, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0569, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9392, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4049, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7674, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3214, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.6692, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9871, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1567, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7789, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1653, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2105, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0075, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9942, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0800, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0220, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8826, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7927, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9471, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3679, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.6034, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9247, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3576, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5047, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9229, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3431, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7257, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1593, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0306, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3579, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6348, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.7993, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2948, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7967, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9562, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7062, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4711, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.6458, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0024, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2553, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.4707, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8783, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.6392, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8723, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0864, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.7444, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1270, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2581, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.7119, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.7113, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0823, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0076, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7225, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3795, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5758, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6567, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1480, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4022, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0705, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1535, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9669, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1198, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0203, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.5423, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(5.0596, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4527, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4233, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5094, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8814, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.6233, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1815, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6547, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3953, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4443, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4911, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5214, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9031, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9364, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1986, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0316, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3759, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5537, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7941, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.6732, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0072, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0070, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2656, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5522, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9224, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9995, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7993, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5120, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5285, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.7345, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8393, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6979, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8015, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9494, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5209, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0553, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0960, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3640, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8331, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9300, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5399, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6610, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2836, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.7617, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4760, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1205, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2358, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0431, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0239, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9009, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3514, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8642, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.7640, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.5117, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8171, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7927, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3917, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1485, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4452, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4617, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2616, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2714, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3447, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2762, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8191, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5983, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2422, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9731, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0176, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6990, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3704, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0363, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3181, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5992, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7829, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4017, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1096, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0331, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0262, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7136, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8626, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7683, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9254, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3253, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6628, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8968, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4636, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.9037, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8138, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2576, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8929, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2965, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4521, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4387, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3049, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2041, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4689, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3900, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5020, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.8388, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8039, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6268, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8853, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4762, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6458, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3748, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.3899, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.0555, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.8233, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0246, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1622, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.3687, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4056, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5464, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.1836, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4321, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4011, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.4220, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.6964, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.2246, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.7955, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5708, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.8940, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.1923, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.0098, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2238, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5452, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(2.9292, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.4793, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(3.5683, grad_fn=<NllLossBackward0>)\n",
      "loss: tensor(4.2422, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Run a single iteration with all different learning rates, store loss for each \n",
    "lri = []\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(600):\n",
    "\n",
    "    ix = torch.randint(0, x.shape[0], (32,))\n",
    "    emb = C[x[ix]] \n",
    "    emb = emb.view(emb.shape[0], emb.shape[1]*emb.shape[2])\n",
    "    layer_1_output = emb @ H + b \n",
    "    logits = layer_1_output @ output_layer + d \n",
    "    loss = CEL(logits, y[ix])\n",
    "    print(\"loss:\", loss)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    lr = lrs[epoch]\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad \n",
    "\n",
    "    if torch.isnan(loss) or loss.item() < 0.9:\n",
    "        break\n",
    "        \n",
    "    lri.append(lrs[epoch])\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5786e5ca-8155-43f4-8190-6241c5c9670d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x72fe8a3dfd10>]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lri, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d22566-8f2b-4ef0-852d-34222cf57d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the plot, loss stagnates or even increases between between 0.02 to 0.025, which is where we want it to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2e1e6f0c-d99d-43c4-9cf8-945dc0e08ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4861056804656982\n",
      "loss: 2.49031400680542\n",
      "loss: 2.4601452350616455\n",
      "loss: 2.9391396045684814\n",
      "loss: 2.5814425945281982\n",
      "loss: 2.4123282432556152\n",
      "loss: 2.634854316711426\n",
      "loss: 2.981149911880493\n",
      "loss: 2.4270946979522705\n",
      "loss: 2.5995395183563232\n",
      "loss: 2.5541577339172363\n",
      "loss: 2.680119752883911\n",
      "loss: 2.565772771835327\n",
      "loss: 2.323140859603882\n",
      "loss: 2.507943630218506\n",
      "loss: 2.379711866378784\n",
      "loss: 2.833301067352295\n",
      "loss: 2.5779857635498047\n",
      "loss: 2.4432907104492188\n",
      "loss: 2.2536983489990234\n",
      "loss: 2.854923725128174\n",
      "loss: 2.877981424331665\n",
      "loss: 2.758111000061035\n",
      "loss: 2.559488296508789\n",
      "loss: 2.677351951599121\n",
      "loss: 2.145857572555542\n",
      "loss: 3.1088790893554688\n",
      "loss: 2.7975997924804688\n",
      "loss: 2.547278881072998\n",
      "loss: 2.549710512161255\n",
      "loss: 2.4045748710632324\n",
      "loss: 2.6268551349639893\n",
      "loss: 2.3909661769866943\n",
      "loss: 2.642761468887329\n",
      "loss: 2.6322567462921143\n",
      "loss: 2.5177180767059326\n",
      "loss: 2.2919845581054688\n",
      "loss: 2.402902126312256\n",
      "loss: 2.5417442321777344\n",
      "loss: 2.3444576263427734\n",
      "loss: 2.3586230278015137\n",
      "loss: 2.157590627670288\n",
      "loss: 2.6864173412323\n",
      "loss: 2.992826223373413\n",
      "loss: 2.5858049392700195\n",
      "loss: 3.011113166809082\n",
      "loss: 2.209423303604126\n",
      "loss: 2.2403955459594727\n",
      "loss: 2.4402148723602295\n",
      "loss: 2.3890888690948486\n",
      "loss: 2.140123128890991\n",
      "loss: 2.4999256134033203\n",
      "loss: 2.0852932929992676\n",
      "loss: 2.572751522064209\n",
      "loss: 2.3182566165924072\n",
      "loss: 2.435973644256592\n",
      "loss: 2.6409451961517334\n",
      "loss: 2.7526655197143555\n",
      "loss: 2.44069242477417\n",
      "loss: 2.645237684249878\n"
     ]
    }
   ],
   "source": [
    "softmax = Softmax(dim=1)\n",
    "CEL = CrossEntropyLoss()\n",
    "epochs = 3000\n",
    "optimizer = optim.SGD([H,output_layer, b,d, C], lr=0.01)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    ix = torch.randint(0, x.shape[0], (32,))\n",
    "    emb = C[x[ix]] \n",
    "    emb = emb.view(emb.shape[0], emb.shape[1]*emb.shape[2])\n",
    "    layer_1_output = emb @ H + b # First layer\n",
    "    logits = layer_1_output @ output_layer + d # Output layer\n",
    "    loss = CEL(logits, y[ix]) # Loss calculation\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() # Calculate how weights affect the loss\n",
    "    optimizer.step() # Change optimization parameters to lower loss\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "245cc406-3ff7-474a-8f71-d922688754e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Learning rate decay\n",
    "### Reduce learning rate as we get closer to global minimum, 0.03 -> 0.001 -> decrease more as we get closer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e2f2fff6-5783-4d5f-820a-47c1a4089730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars:  ['n', 't', 'i']\n",
      "next:  n\n",
      "prediction:  a\n",
      "\n",
      "\n",
      "chars:  ['e', 'm', 'm']\n",
      "next:  y\n",
      "prediction:  a\n",
      "\n",
      "\n",
      "chars:  ['.', '.', '.']\n",
      "next:  k\n",
      "prediction:  .\n",
      "\n",
      "\n",
      "chars:  ['r', 'a', 'h']\n",
      "next:  .\n",
      "prediction:  a\n",
      "\n",
      "\n",
      "chars:  ['z', 'a', 'e']\n",
      "next:  l\n",
      "prediction:  .\n",
      "\n",
      "\n",
      "chars:  ['.', '.', '.']\n",
      "next:  m\n",
      "prediction:  a\n",
      "\n",
      "\n",
      "chars:  ['.', 'n', 'i']\n",
      "next:  a\n",
      "prediction:  a\n",
      "\n",
      "\n",
      "chars:  ['.', '.', 'c']\n",
      "next:  a\n",
      "prediction:  a\n",
      "\n",
      "\n",
      "chars:  ['.', '.', 'm']\n",
      "next:  o\n",
      "prediction:  a\n",
      "\n",
      "\n",
      "chars:  ['l', 'a', 'i']\n",
      "next:  n\n",
      "prediction:  a\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "\n",
    "\n",
    "ix = torch.randint(0, x.shape[0], (20,))\n",
    "emb = C[x[ix]]\n",
    "emb = emb.view(emb.shape[0], emb.shape[1]*emb.shape[2])\n",
    "layer_1_output = emb @ H + b # First layer\n",
    "logits = layer_1_output @ output_layer + d # Output layer\n",
    "\n",
    "labels = y[ix]\n",
    "\n",
    "for i in range(len(prob)):\n",
    "    max = torch.argmax(prob[i]) # Gets highest probability\n",
    "    chars = x[ix][i]\n",
    "    chars = [itoi[char.item()] for char in chars]\n",
    "    print(\"chars: \", chars)\n",
    "    print(\"next: \", itoi[labels[i].item()])\n",
    "    print(\"prediction: \", itoi[max.item()])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba6208-b14a-45d2-81e0-14251fa296ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training split and use cases\n",
    "\"\"\"\n",
    "Training set: use to optimize parameters of the model -> [H,W,C,d,b]\n",
    "Dev/Validation set: use to optimize hyperparameters -> size of hidden layer, embedding size, regularization strength\n",
    "Test set: final testing\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "ded8d7cd-8998-4e7e-b0a1-881d2c360ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eri.\n",
      "ammyanhheen.\n",
      "ndhey.\n",
      "marenhrstend.\n",
      "len.\n",
      "adeeed.\n",
      "elii.\n",
      "emi.\n",
      "jen.\n",
      "edeisean.\n",
      "xar.\n",
      "emy.\n",
      "koshara.\n",
      "n.\n",
      "sadbergahiries.\n",
      "jin.\n",
      "renel.\n",
      "pantenof.\n",
      "ube.\n",
      "tede.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "block_size = 3\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      emb = emb.view(emb.shape[0], emb.shape[1]*emb.shape[2])\n",
    "      layer_1_output = emb @ H + b # First layer\n",
    "      logits = layer_1_output @ output_layer + d # Output layer\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itoi[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "7af543ce-3c45-4502-8c85-793789c776e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project\n",
    "\n",
    "# implement NPL using class and make it traininable on random text using a python file\n",
    "# Have it generate random words of length specified by user using user's first 3 input words, then generating the next word from\n",
    "# 3 last words as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f066f-135f-4378-9942-4cdd4f747a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
