{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdbb44d5-88ee-47a8-a1c2-0cccdcc9680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d370d8b-5802-4437-932f-d063dc2cbe8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny = b + Wx + U * tanh(d + Hx)\\n\\nx = concat of all input sequence feature vectors(words)\\nb = biases for W\\nd = biases for H\\nW = direct representation matrix\\nH = hidden layer matrix\\nU = another hidden to output layer matrix\\n\\ny = (Wx + b) + (U * tanh(d+Hx))\\ny =  (1,|V|) +   (1, |V|) \\n     \\ngoes to two different models, addition = (1,|V|) + (1, |V|) = (1,|V|)\\n|V| -> length of vocabuluary\\n\\nthen (1,|V|) -> softmax -> probabilities for each word in vocab\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "\"\"\"\n",
    "y = b + Wx + U * tanh(d + Hx)\n",
    "\n",
    "x = concat of all input sequence feature vectors(words)\n",
    "b = biases for W\n",
    "d = biases for H\n",
    "W = direct representation matrix\n",
    "H = hidden layer matrix\n",
    "U = another hidden to output layer matrix\n",
    "\n",
    "y = (Wx + b) + (U * tanh(d+Hx))\n",
    "y =  (1,|V|) +   (1, |V|) \n",
    "     \n",
    "goes to two different models, addition = (1,|V|) + (1, |V|) = (1,|V|)\n",
    "|V| -> length of vocabuluary\n",
    "\n",
    "then (1,|V|) -> softmax -> probabilities for each word in vocab\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a2286ce-381f-4659-90d8-a959e72f2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPL:\n",
    "\n",
    "    def __init__(self, vocab, hidden_units=100, context_size=3, feature_word_len=10, has_direct_rep=True):\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.feature_word_len = feature_word_len\n",
    "        self.has_direct_rep = has_direct_rep\n",
    "        self.context_size = context_size\n",
    "        self.vocab = vocab\n",
    "\n",
    "\n",
    "        self.C = torch.randn(len(self.vocab), feature_word_len)\n",
    "        self.hidden_layer = torch.randn((self.context_size*self.feature_word_len), self.hidden_units)\n",
    "        self.b = torch.randn(self.hidden_units)\n",
    "        self.output_layer = torch.randn(self.hidden_units, len(self.vocab))\n",
    "        \n",
    "        self.parameters = [self.C, self.hidden_layer, self.b, self.output_layer]\n",
    "        \n",
    "        if has_direct_rep:\n",
    "            self.direct_representation = torch.randn((self.context_size*self.feature_word_len), len(self.vocab))\n",
    "            self.d = torch.randn(len(self.vocab))\n",
    "            self.parameters.extend([self.direct_representation, self.d])\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.CLE = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Set parameters gradient to true\n",
    "        for p in self.parameters:\n",
    "            p.requires_grad = True\n",
    "            \n",
    "    # List of word indexes to feature vectors\n",
    "    def get_feature_vectors(self, x):\n",
    "\n",
    "        # C[[index_1,index_2,index_3],...]\n",
    "        x = self.C[x]\n",
    "       \n",
    "        # concat all input feature vectors into one\n",
    "        x = x.view(x.shape[0], x.shape[1]*x.shape[2]) # [B, context_size*feature_vector_len)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.get_feature_vectors(x)\n",
    "        #print(x.shape,y.shape) # [B, context_size*feature_vector_len] , [B, feature_vector_len]\n",
    "        \n",
    "        # Hidden layer tanh(b+Hx)\n",
    "        H = self.tanh(torch.matmul(x, self.hidden_layer) + self.b)\n",
    "        O = torch.matmul(H, self.output_layer) # [B, |vocab|]\n",
    "\n",
    "        if self.has_direct_rep:\n",
    "            # Direct representation layer (Wx + d)\n",
    "            D = torch.matmul(x, self.direct_representation) + self.d\n",
    "            logits = O + D\n",
    "        else:\n",
    "            logits = O\n",
    "\n",
    "        return logits\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        logits = self.forward(x)\n",
    "        return logits\n",
    "        \n",
    "    def generate(self, start_context, length):\n",
    "\n",
    "        if type(start_context) is not str:\n",
    "            raise \"Context has to be a string\"        \n",
    "\n",
    "        start_context = start_context.split()\n",
    "\n",
    "        if len(start_context) > self.context_size:\n",
    "            print(\"input string larger than context size, might lead to improper responses\\n\")\n",
    "\n",
    "        elif len(start_context) < self.context_size:\n",
    "            raise f\"Input needs to be atleast {self.context_size} words\"\n",
    "\n",
    "        generated_text = start_context\n",
    "        current_context = start_context[-self.context_size:]\n",
    "        \n",
    "        for i in range(length):\n",
    "\n",
    "            index_vectors = torch.tensor([[self.vocab.index(word) for word in current_context]])\n",
    "            logits = self.forward(index_vectors)\n",
    "            prob = self.softmax(logits)\n",
    "            next_pred = self.vocab[torch.argmax(prob)]\n",
    "            generated_text.append(next_pred)\n",
    "            current_context = generated_text[-self.context_size:]\n",
    "            \n",
    "        return ' '.join(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "392df478-5ba1-49d0-97e6-d666fa20f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_text(filename):\n",
    "    words = open(filename, \"r\").read()\n",
    "    words = words.lower()\n",
    "    words = re.sub(r'[^a-zA-Z\\s]', '', words)\n",
    "    words = words.split()\n",
    "    \n",
    "    # Create vocabulary with all unique words in text file\n",
    "    vocab = sorted(list(set(words)))\n",
    "\n",
    "    return words, vocab\n",
    "    \n",
    "def create_pairs(words, context_size):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(words)-context_size):\n",
    "        x.append(words[i:i+context_size])\n",
    "        y.append(words[i+context_size:i+context_size+1])\n",
    "\n",
    "    # x[i] -> [\"asd\",\"Asd\",\"aw\"] context_size=3\n",
    "    # y[i] -> [\"fgds\"]\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def get_index_vectors(x, y, words_to_i):\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[i])):\n",
    "            x[i][j] = words_to_i[x[i][j]]\n",
    "        y[i] = words_to_i[y[i][0]]\n",
    "\n",
    "    # x -> [12312,1231,1] context_size=3\n",
    "    # y -> [5]\n",
    "    return x,y\n",
    "\n",
    "def get_word_dict(vocab):\n",
    "    \n",
    "    words_to_i = {}\n",
    "    i_to_words = {}\n",
    "    \n",
    "    for i in enumerate(vocab):\n",
    "        #('word', index) <=> (index, 'word')\n",
    "        words_to_i[i[1]] = i[0]\n",
    "        i_to_words[i[0]] = i[1]\n",
    "        \n",
    "    return words_to_i, i_to_words\n",
    "    \n",
    "def train(text_file, **kwargs):\n",
    "    \n",
    "    defaults = {\n",
    "        'hidden_units': 100,\n",
    "        'context_size': 3,\n",
    "        'feature_vector_size': 10,\n",
    "        'direct_rep': False,\n",
    "        'epochs': 50,\n",
    "    }\n",
    "\n",
    "    defaults.update(kwargs)\n",
    "\n",
    "    # Prepare data\n",
    "    words, vocab = prepare_text(text_file)\n",
    "\n",
    "    # Helper dictionaries mapping words to index and vice versa\n",
    "    words_to_i, i_to_words = get_word_dict(vocab)\n",
    "    \n",
    "    x,y = create_pairs(words, defaults['context_size'])\n",
    "    x,y = get_index_vectors(x,y, words_to_i)\n",
    "\n",
    "    x,y = torch.tensor(x), torch.tensor(y)\n",
    "    # Model\n",
    "    model = NPL(vocab=vocab, hidden_units=defaults['hidden_units'], context_size=defaults['context_size'], \n",
    "                feature_word_len=defaults['feature_vector_size'], has_direct_rep=defaults['direct_rep'])\n",
    "\n",
    "    # optimizer and loss\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    CLE = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters, lr=0.01, momentum=0.9)\n",
    "\n",
    "    for epoch in range(defaults['epochs']):\n",
    "\n",
    "        # Random 50 indexes \n",
    "        res = random.sample(range(0, x.shape[0]), 50)\n",
    "        batch_x = x[res]\n",
    "        batch_y = y[res]\n",
    "\n",
    "        logits = model(batch_x)\n",
    "        loss = CLE(logits, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86f61389-ebc8-43ce-b59d-72ed5539322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(\"t8.shakespeare.txt\", epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2539a58f-371a-429f-a06e-ba93589d1144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is the other permanent safely threefarthings pyramises medicine grasps no curd levitys'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\"this is the\", length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233404be-f186-4865-a496-84444ca64db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import NPL, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a90aefa-c4d5-4eb8-bed4-7d7b6eafbb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "npl_model = train(\"t8.shakespeare.txt\", epochs=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbdcc66e-79ce-4ead-bc53-bdc4809108d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "npl_model_2 = train(\"t8.shakespeare.txt\", epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c966def-2c96-4082-b08f-fc0551fbc262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
